{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771bc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7627b6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Grid Search Combination 1/6 =====\n",
      "Learning Rate: 0.001, Batch Size: 8, Dropout: 0.2, Weight Decay: 1e-06\n",
      "\n",
      "===== Fold 1 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 2 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 3 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 4 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 5 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 6 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 7 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 8 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 9 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 10 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 11 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 12 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 13 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 14 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 15 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 16 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 17 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 18 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 19 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 20 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Grid Search Combination 2/6 =====\n",
      "Learning Rate: 0.001, Batch Size: 8, Dropout: 0.2, Weight Decay: 1e-05\n",
      "\n",
      "===== Fold 1 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 2 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 3 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 4 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 5 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 6 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 7 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 8 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 9 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 10 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 11 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 12 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 13 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 14 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 15 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 16 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 17 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 18 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 19 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 20 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Grid Search Combination 3/6 =====\n",
      "Learning Rate: 0.001, Batch Size: 8, Dropout: 0.2, Weight Decay: 0.0001\n",
      "\n",
      "===== Fold 1 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 2 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 3 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 4 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 5 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 6 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 7 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 8 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 9 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 10 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 11 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 12 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 13 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 14 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 15 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 16 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 17 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 18 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 19 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 20 / 20 for Hyperparameters (lr=0.001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Grid Search Combination 4/6 =====\n",
      "Learning Rate: 0.0001, Batch Size: 8, Dropout: 0.2, Weight Decay: 1e-06\n",
      "\n",
      "===== Fold 1 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 2 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 3 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 4 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 5 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 6 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 7 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 8 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 9 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 10 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 11 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 12 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 13 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 14 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 15 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 16 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 17 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 18 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 19 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Fold 20 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-06) =====\n",
      "\n",
      "===== Grid Search Combination 5/6 =====\n",
      "Learning Rate: 0.0001, Batch Size: 8, Dropout: 0.2, Weight Decay: 1e-05\n",
      "\n",
      "===== Fold 1 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 2 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 3 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 4 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 5 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 6 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 7 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 8 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 9 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 10 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 11 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 12 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 13 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 14 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 15 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 16 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 17 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 18 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 19 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Fold 20 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=1e-05) =====\n",
      "\n",
      "===== Grid Search Combination 6/6 =====\n",
      "Learning Rate: 0.0001, Batch Size: 8, Dropout: 0.2, Weight Decay: 0.0001\n",
      "\n",
      "===== Fold 1 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 2 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 3 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 4 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 5 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 6 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 7 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 8 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 9 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 10 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 11 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 12 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 13 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 14 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 15 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 16 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 17 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 18 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 19 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Fold 20 / 20 for Hyperparameters (lr=0.0001, batchsize=8, dd=0.2, wd=0.0001) =====\n",
      "\n",
      "===== Grid Search Completed =====\n",
      "   Learning_Rate  Batch_Size  Dropout  Weight_Decay  Mean_AUC_ROC  \\\n",
      "0         0.0010           8      0.2      0.000001      0.730015   \n",
      "1         0.0010           8      0.2      0.000010      0.769290   \n",
      "2         0.0010           8      0.2      0.000100      0.740664   \n",
      "3         0.0001           8      0.2      0.000001      0.745949   \n",
      "4         0.0001           8      0.2      0.000010      0.696373   \n",
      "5         0.0001           8      0.2      0.000100      0.733179   \n",
      "\n",
      "   Std_AUC_ROC  Mean_AUC_PR  Std_AUC_PR  \n",
      "0     0.117634     0.768143    0.096235  \n",
      "1     0.124013     0.782985    0.131185  \n",
      "2     0.101772     0.791378    0.086057  \n",
      "3     0.143742     0.767693    0.144781  \n",
      "4     0.110719     0.723575    0.123823  \n",
      "5     0.131501     0.764990    0.123661  \n"
     ]
    }
   ],
   "source": [
    "# get drug features using Deepchem library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import argparse\n",
    "import random, sys\n",
    "import numpy as np\n",
    "import csv\n",
    "from scipy import stats\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES before importing TensorFlow and Keras\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Chemical_Genotoxicity_pre')\n",
    "    parser.add_argument('-gpu_id', dest='gpu_id', type=str, default='0', help='GPU devices')\n",
    "    parser.add_argument('-israndom', dest='israndom', type=bool, default=False, help='randomlize X and A')\n",
    "    # Hyperparameters for GCN\n",
    "    parser.add_argument('-unit_list', dest='unit_list', nargs='+', type=int, default=[256, 256, 256],\n",
    "                        help='unit list for GCN')\n",
    "    parser.add_argument('-use_bn', dest='use_bn', type=bool, default=True, help='use batch normalization for GCN')\n",
    "    parser.add_argument('-use_relu', dest='use_relu', type=bool, default=True, help='use relu for GCN')\n",
    "    parser.add_argument('-use_GMP', dest='use_GMP', type=bool, default=True, help='use GlobalMaxPooling for GCN')\n",
    "    #fold수 설정 =30\n",
    "    parser.add_argument('-n_splits', dest='n_splits', type=int, default=20, help='Number of cross-validation folds')\n",
    "    parser.add_argument('-random_state', dest='random_state', type=int, default=1, help='Random state for reproducibility')\n",
    "    \n",
    "    # parse_known_args를 사용하여 알 수 없는 인수를 무시\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to use GPU0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
    "\n",
    "# Now import TensorFlow and Keras after setting the environment variable\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, InputLayer, Multiply, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Lambda\n",
    "from keras import optimizers, utils\n",
    "from keras.constraints import max_norm\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, History, CSVLogger, ReduceLROnPlateau\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.stats import pearsonr\n",
    "from Car_model import KerasMultiSourceGCNModel\n",
    "import hickle as hkl\n",
    "import scipy.sparse as sp\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds and TensorFlow configurations\n",
    "random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(0)\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "israndom = args.israndom\n",
    "GCN_deploy = '_'.join(map(str, args.unit_list)) + '_' + ('bn' if args.use_bn else 'no_bn') + '_' + ('relu' if args.use_relu else 'tanh') + '_' + ('GMP' if args.use_GMP else 'GAP')\n",
    "model_suffix = GCN_deploy\n",
    "\n",
    "####################################Constants Settings###########################\n",
    "Drug_feature_file = r'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/CarcGC_data/drug_graph_feat/'\n",
    "Max_atoms = 100\n",
    "\n",
    "def DataGenerate(Drug_feature_file):\n",
    "    # load drug features\n",
    "    drug_pubchem_id_set = []\n",
    "    all_drug_feature = {}\n",
    "    for each in os.listdir(Drug_feature_file):\n",
    "        # 디렉토리가 아닌 파일만 처리\n",
    "        if os.path.isfile(os.path.join(Drug_feature_file, each)):\n",
    "            drug_pubchem_id_set.append(each.split('.')[0])\n",
    "            feat_mat, adj_list, degree_list = hkl.load(os.path.join(Drug_feature_file, each))\n",
    "            all_drug_feature[each.split('.')[0]] = [feat_mat, adj_list, degree_list]\n",
    "    assert len(drug_pubchem_id_set) == len(all_drug_feature.values())\n",
    "    return all_drug_feature\n",
    "\n",
    "def MetadataGenerate(all_drug_feature, train_csv_path):\n",
    "    label = pd.read_csv(train_csv_path, index_col=None, header=0)\n",
    "    drugnames = label['pert_id'].tolist()\n",
    "    label['Carcinogenicity_label'] = (label['Carcinogenicity'] == '+').astype(int)\n",
    "    data_idx = list(zip(label['pert_id'], label['Carcinogenicity_label']))\n",
    "    nb_drugs = len(set([item[0] for item in data_idx]))\n",
    "    # Ensure that all drugnames exist in all_drug_feature\n",
    "    valid_drugnames = [drug for drug in drugnames if drug in all_drug_feature]\n",
    "    if len(valid_drugnames) < len(drugnames):\n",
    "        missing_drugs = set(drugnames) - set(valid_drugnames)\n",
    "        print(f\"Warning: The following pert_id are missing in drug_graph_feat and will be skipped: {missing_drugs}\")\n",
    "    drug_feature = {key: all_drug_feature[key] for key in valid_drugnames}\n",
    "    data_idx = [item for item in data_idx if item[0] in valid_drugnames]\n",
    "    return drug_feature, data_idx\n",
    "\n",
    "def ValidationGenerate(all_drug_feature, val_csv_path):\n",
    "    label = pd.read_csv(val_csv_path, index_col=None, header=0)\n",
    "    drugnames = label['pert_id'].tolist()\n",
    "    label['Carcinogenicity_label'] = (label['Carcinogenicity'] == '+').astype(int)\n",
    "    data_idx = list(zip(label['pert_id'], label['Carcinogenicity_label']))\n",
    "    nb_drugs = len(set([item[0] for item in data_idx]))\n",
    "    # Ensure that all drugnames exist in all_drug_feature\n",
    "    valid_drugnames = [drug for drug in drugnames if drug in all_drug_feature]\n",
    "    if len(valid_drugnames) < len(drugnames):\n",
    "        missing_drugs = set(drugnames) - set(valid_drugnames)\n",
    "        print(f\"Warning: The following pert_id are missing in drug_graph_feat and will be skipped: {missing_drugs}\")\n",
    "    drug_feature = {key: all_drug_feature[key] for key in valid_drugnames}\n",
    "    data_idx = [item for item in data_idx if item[0] in valid_drugnames]\n",
    "    return drug_feature, data_idx\n",
    "\n",
    "def ExtraGenerate(all_drug_feature, test_csv_path):\n",
    "    label = pd.read_csv(test_csv_path, index_col=None, header=0)\n",
    "    drugnames = label['pert_id'].tolist()\n",
    "    label['Carcinogenicity_label'] = (label['Carcinogenicity'] == '+').astype(int)\n",
    "    data_idx = list(zip(label['pert_id'], label['Carcinogenicity_label']))\n",
    "    nb_drugs = len(set([item[0] for item in data_idx]))\n",
    "    # Ensure that all drugnames exist in all_drug_feature\n",
    "    valid_drugnames = [drug for drug in drugnames if drug in all_drug_feature]\n",
    "    if len(valid_drugnames) < len(drugnames):\n",
    "        missing_drugs = set(drugnames) - set(valid_drugnames)\n",
    "        print(f\"Warning: The following pert_id are missing in drug_graph_feat and will be skipped: {missing_drugs}\")\n",
    "    drug_feature = {key: all_drug_feature[key] for key in valid_drugnames}\n",
    "    data_idx = [item for item in data_idx if item[0] in valid_drugnames]\n",
    "    return drug_feature, data_idx\n",
    "\n",
    "def NormalizeAdj(adj):\n",
    "    adj = adj + np.eye(adj.shape[0])\n",
    "    d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0).toarray()\n",
    "    a_norm = adj.dot(d).transpose().dot(d)\n",
    "    return a_norm\n",
    "\n",
    "def random_adjacency_matrix(n):\n",
    "    matrix = [[random.randint(0, 1) for i in range(n)] for j in range(n)]\n",
    "    # No vertex connects to itself\n",
    "    for i in range(n):\n",
    "        matrix[i][i] = 0\n",
    "    # If i is connected to j, j is connected to i\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            matrix[j][i] = matrix[i][j]\n",
    "    return matrix\n",
    "\n",
    "def CalculateGraphFeat(feat_mat, adj_list):\n",
    "    # Truncate feat_mat and adj_list if they exceed Max_atoms\n",
    "    if feat_mat.shape[0] > Max_atoms:\n",
    "#         print(f\"Warning: Truncating feat_mat from {feat_mat.shape[0]} to {Max_atoms}.\")\n",
    "        feat_mat = feat_mat[:Max_atoms]\n",
    "        adj_list = adj_list[:Max_atoms]\n",
    "    \n",
    "    # Ensure the number of nodes in feat_mat matches the length of adj_list\n",
    "    assert feat_mat.shape[0] == len(adj_list)\n",
    "    \n",
    "    # Initialize feature and adjacency matrices\n",
    "    feat = np.zeros((Max_atoms, feat_mat.shape[-1]), dtype='float32')\n",
    "    adj_mat = np.zeros((Max_atoms, Max_atoms), dtype='float32')\n",
    "    \n",
    "    # If israndom is set, generate random feature and adjacency matrices\n",
    "    if israndom:\n",
    "        feat = np.random.rand(Max_atoms, feat_mat.shape[-1])\n",
    "        adj_mat[feat_mat.shape[0]:, feat_mat.shape[0]:] = random_adjacency_matrix(Max_atoms - feat_mat.shape[0])\n",
    "    \n",
    "    # Fill the feature matrix with feat_mat\n",
    "    feat[:feat_mat.shape[0], :] = feat_mat\n",
    "    \n",
    "    # Construct the adjacency matrix\n",
    "    for i in range(len(adj_list)):\n",
    "        nodes = adj_list[i]\n",
    "        for each in nodes:\n",
    "            if each < Max_atoms:  # Ensure node index is within bounds\n",
    "                adj_mat[i, int(each)] = 1\n",
    "    # Ensure the adjacency matrix is symmetric\n",
    "    assert np.allclose(adj_mat, adj_mat.T)\n",
    "    adj_ = adj_mat[:len(adj_list), :len(adj_list)]\n",
    "    adj_2 = adj_mat[len(adj_list):, len(adj_list):]\n",
    "    norm_adj_ = NormalizeAdj(adj_)\n",
    "    norm_adj_2 = NormalizeAdj(adj_2)\n",
    "    adj_mat[:len(adj_list), :len(adj_list)] = norm_adj_\n",
    "    adj_mat[len(adj_list):, len(adj_list):] = norm_adj_2\n",
    "    return [feat, adj_mat]\n",
    "\n",
    "def FeatureExtract(data_idx, drug_feature):\n",
    "    nb_instance = len(data_idx)\n",
    "    drug_data = [[] for _ in range(nb_instance)]\n",
    "    target = np.zeros(nb_instance, dtype='int16')\n",
    "    for idx in range(nb_instance):\n",
    "        drugname, clabel = data_idx[idx]\n",
    "        # modify\n",
    "        feat_mat, adj_list, _ = drug_feature[str(drugname)]\n",
    "        # fill drug data, padding to the same size with zeros\n",
    "        drug_data[idx] = CalculateGraphFeat(feat_mat, adj_list)\n",
    "        # randomlize X A\n",
    "        target[idx] = clabel\n",
    "    # return drug_data, target\n",
    "    return drug_data, target\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    recal = recall_metric(y_true, y_pred)\n",
    "    return 2.0 * prec * recal / (prec + recal + K.epsilon())\n",
    "\n",
    "def average_precision_metric(y_true, y_pred):\n",
    "    return tf.py_function(average_precision_score, (y_true, y_pred), tf.double) \n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, training_data, validation_data, patience, fold):\n",
    "        self.x_train = training_data[0]\n",
    "        self.y_train = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        self.best_weight = None\n",
    "        self.patience = patience\n",
    "        self.fold = fold  # To differentiate between folds\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = -np.Inf\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.auct = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.aucl = {'batch': [], 'epoch': []}\n",
    "        self.H = {}\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weight is not None:\n",
    "            self.model.set_weights(self.best_weight)\n",
    "            self.model.save(f'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/CarcGC_data/bestmodel/BestCarcGC_Cartoxicity_classify_fold{self.fold}_{model_suffix}.h5')\n",
    "#         if self.stopped_epoch > 0:\n",
    "#             print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        y_pred_train = self.model.predict(self.x_train)\n",
    "        roc_train = roc_auc_score(self.y_train, y_pred_train)\n",
    "        precision, recall, _ = metrics.precision_recall_curve(self.y_val, y_pred_val)\n",
    "        pr_val = metrics.average_precision_score(self.y_val, y_pred_val)\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.auct['epoch'].append(roc_train)\n",
    "        self.aucl['epoch'].append(roc_val)\n",
    "#         print(f'Fold {self.fold} - Epoch {epoch+1}: roc-val: {roc_val:.4f}, pr-val: {pr_val:.4f}')\n",
    "        if roc_val > self.best:\n",
    "            self.best = roc_val\n",
    "            self.wait = 0\n",
    "            self.best_weight = self.model.get_weights()\n",
    "            self.model.save(f'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/CarcGC_data/bestmodel/BestCarcGC_Cartoxicity_highestAUCROC_fold{self.fold}_{model_suffix}.h5')\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "        return\n",
    "\n",
    "    def savedata(self, lr, batchsize, wd, dd, fold):\n",
    "        iters = range(len(self.val_loss['epoch']))\n",
    "        eponb = float(len(self.losses['batch'])) / float(len(self.val_loss['epoch']))\n",
    "        dflist = []\n",
    "        for ii in iters:\n",
    "            ystart = int(ii * eponb)\n",
    "            yend = int((ii + 1) * eponb)\n",
    "            yloss = self.losses['epoch'][ii]\n",
    "            valloss = self.val_loss['epoch'][ii]\n",
    "            aucroct = self.auct['epoch'][ii]\n",
    "            aucroc = self.aucl['epoch'][ii]\n",
    "            dflist.append([ii + 1, aucroct, aucroc, yloss, valloss])\n",
    "        df = pd.DataFrame(dflist)\n",
    "        df.columns = ['epoch', 'auc_train', 'auc_val', 'train_loss', 'validation_loss']\n",
    "        df.to_csv(f'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/CarcGC_data/gridsearch_loss/lr{lr}_batch{batchsize}_dropout{dd}_{wd}_fold{fold}_loss.csv', index=False, header=True)\n",
    "        return\n",
    "\n",
    "def ModelTraining(model, lr, batchsize, wd, dd, X_drug_data_train, Y_train, validation_data, patience, fold, nb_epoch=500):\n",
    "    optimizer = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', precision_metric, recall_metric, f1_score_metric, average_precision_metric])\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_drug_feat_data_train = np.array([item[0] for item in X_drug_data_train])  # nb_instance * Max_atoms * feat_dim\n",
    "    X_drug_adj_data_train = np.array([item[1] for item in X_drug_data_train])   # nb_instance * Max_atoms * Max_atoms\n",
    "    \n",
    "    # Unpack validation data correctly\n",
    "    X_val_data, Y_val = validation_data\n",
    "    X_val_feat, X_val_adj = zip(*X_val_data)\n",
    "    X_val_feat = np.array(X_val_feat)\n",
    "    X_val_adj = np.array(X_val_adj)\n",
    "    \n",
    "    history = MyCallback(training_data=[[X_drug_feat_data_train, X_drug_adj_data_train], Y_train],\n",
    "                         validation_data=([X_val_feat, X_val_adj], Y_val),\n",
    "                         patience=patience,\n",
    "                         fold=fold)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=f'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/CarcGC_data/checkpoint_weight/Cartoxicity_weights_fold{fold}_{{epoch:04d}}.h5', verbose=0),\n",
    "        history\n",
    "    ]\n",
    "    model.fit(x=[X_drug_feat_data_train, X_drug_adj_data_train],\n",
    "              validation_data=([X_val_feat, X_val_adj], Y_val),\n",
    "              y=Y_train,\n",
    "              batch_size=batchsize,\n",
    "              epochs=nb_epoch,\n",
    "              callbacks=callbacks,verbose=0)\n",
    "    history.savedata(lr, batchsize, wd, dd, fold)\n",
    "    return model\n",
    "\n",
    "def ModelEvaluate(model, X_drug_data_test, Y_test, log_file_path):\n",
    "    X_drug_feat_data_test = np.array([item[0] for item in X_drug_data_test])  # nb_instance * Max_atoms * feat_dim\n",
    "    X_drug_adj_data_test = np.array([item[1] for item in X_drug_data_test])   # nb_instance * Max_atoms * Max_atoms    \n",
    "    Y_pred = model.predict([X_drug_feat_data_test, X_drug_adj_data_test])\n",
    "    auROC_all = metrics.roc_auc_score(Y_test, Y_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(Y_test, Y_pred)\n",
    "    precision, recall, _ = metrics.precision_recall_curve(Y_test, Y_pred)\n",
    "    auPR_all = metrics.average_precision_score(Y_test, Y_pred)\n",
    "#     print(\"The overall AUC and auPR is %.4f and %.4f.\" % (auROC_all, auPR_all))\n",
    "    \n",
    "    # Optionally, log the results\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write(f\"AUC-ROC: {auROC_all:.4f}\\n\")\n",
    "        f.write(f\"AUC-PR: {auPR_all:.4f}\\n\")\n",
    "    \n",
    "    return auROC_all, auPR_all, Y_pred\n",
    "\n",
    "def main():\n",
    "    # Define hyperparameter grid\n",
    "    lr_list = [0.001, 0.0001]\n",
    "    batchsize_list = [8]\n",
    "    dd_list = [0.2]\n",
    "    wd_list = [1e-06, 1e-05, 1e-04]\n",
    "    \n",
    "    # Initialize a list to store grid search results\n",
    "    grid_results = []\n",
    "    \n",
    "    # Total number of combinations for progress tracking (optional)\n",
    "    total_combinations = len(lr_list) * len(batchsize_list) * len(dd_list) * len(wd_list)\n",
    "    current_combination = 1\n",
    "    \n",
    "    for lr in lr_list:\n",
    "        for batchsize in batchsize_list:\n",
    "            for dd in dd_list:\n",
    "                for wd in wd_list:\n",
    "                    print(f\"\\n===== Grid Search Combination {current_combination}/{total_combinations} =====\")\n",
    "                    print(f\"Learning Rate: {lr}, Batch Size: {batchsize}, Dropout: {dd}, Weight Decay: {wd}\")\n",
    "                    \n",
    "                    # Set seeds for reproducibility\n",
    "                    random.seed(1)\n",
    "                    tf.set_random_seed(2)\n",
    "                    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "                    np.random.seed(2)\n",
    "                    \n",
    "                    # Load drug features\n",
    "                    all_drug_feature = DataGenerate(Drug_feature_file)\n",
    "                    \n",
    "                    # Load train and test data\n",
    "                    train_csv_path = r'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/Dataset/mgl_train_data.csv'\n",
    "                    val_csv_path = r'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/Dataset/mgl_val_data.csv'\n",
    "                    test_csv_path = r'/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/Dataset/mgl_test_data.csv'\n",
    "                    drug_feature_train, data_idx_train = MetadataGenerate(all_drug_feature, train_csv_path)\n",
    "                    drug_feature_val, data_idx_val = ValidationGenerate(all_drug_feature, val_csv_path)    \n",
    "                    drug_feature_test, data_idx_test = ExtraGenerate(all_drug_feature, test_csv_path)\n",
    "                    \n",
    "                    # Combine train and test data\n",
    "                    combined_drug_feature = {**drug_feature_train, **drug_feature_val, **drug_feature_test}\n",
    "                    combined_data_idx = data_idx_train + data_idx_val + data_idx_test\n",
    "                    \n",
    "                    # Extract features\n",
    "                    X_drug_data, Y = FeatureExtract(combined_data_idx, combined_drug_feature)\n",
    "                    \n",
    "                    # Convert to numpy arrays\n",
    "                    X_feat = np.array([item[0] for item in X_drug_data])  # nb_instance * Max_atoms * feat_dim\n",
    "                    X_adj = np.array([item[1] for item in X_drug_data])   # nb_instance * Max_atoms * Max_atoms\n",
    "                    Y = np.array(Y)\n",
    "                    \n",
    "                    # Determine drug_dim\n",
    "                    drug_dim = X_feat.shape[-1]\n",
    "                    \n",
    "                    # Define Stratified K-Fold\n",
    "                    n_splits = args.n_splits\n",
    "                    random_state = args.random_state\n",
    "                    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "                    \n",
    "                    # Initialize metrics lists for this hyperparameter combination\n",
    "                    auROC_folds = []\n",
    "                    auPR_folds = []\n",
    "                    \n",
    "                    fold_num = 1\n",
    "                    for outer_train_idx, outer_test_idx in skf.split(X_feat, Y):\n",
    "                        print(f\"\\n===== Fold {fold_num} / {n_splits} for Hyperparameters (lr={lr}, batchsize={batchsize}, dd={dd}, wd={wd}) =====\")\n",
    "                        \n",
    "                        # Split into outer train and outer test\n",
    "                        X_outer_train_feat, X_outer_test_feat = X_feat[outer_train_idx], X_feat[outer_test_idx]\n",
    "                        X_outer_train_adj, X_outer_test_adj = X_adj[outer_train_idx], X_adj[outer_test_idx]\n",
    "                        Y_outer_train, Y_outer_test = Y[outer_train_idx], Y[outer_test_idx]\n",
    "                        \n",
    "                        # From outer_train, create inner train and validation using Stratified Shuffle Split\n",
    "                        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=random_state)\n",
    "                        for inner_train_idx, inner_val_idx in sss.split(X_outer_train_feat, Y_outer_train):\n",
    "                            X_inner_train_feat, X_val_feat = X_outer_train_feat[inner_train_idx], X_outer_train_feat[inner_val_idx]\n",
    "                            X_inner_train_adj, X_val_adj = X_outer_train_adj[inner_train_idx], X_outer_train_adj[inner_val_idx]\n",
    "                            Y_inner_train, Y_val = Y_outer_train[inner_train_idx], Y_outer_train[inner_val_idx]\n",
    "                        \n",
    "                        # Prepare training and validation data\n",
    "                        X_inner_train_data = list(zip(X_inner_train_feat, X_inner_train_adj))\n",
    "                        X_val_data = list(zip(X_val_feat, X_val_adj))\n",
    "                        \n",
    "                        # Create model\n",
    "                        try:\n",
    "                            model = KerasMultiSourceGCNModel(regr=False).createMaster(\n",
    "                                drug_dim=drug_dim,\n",
    "                                units_list=args.unit_list,\n",
    "                                wd=wd,\n",
    "                                dd=dd,\n",
    "                                use_relu=args.use_relu,\n",
    "                                use_bn=args.use_bn,\n",
    "                                use_GMP=args.use_GMP\n",
    "                            )\n",
    "                        except TypeError as e:\n",
    "                            print(f\"Error creating model in fold {fold_num}: {e}\")\n",
    "                            print(\"Please check the 'createMaster' method in 'Car_model.py' and ensure that the arguments match.\")\n",
    "                            sys.exit(1)\n",
    "                        \n",
    "                        # Train model\n",
    "#                         print('Begin training...')\n",
    "                        model = ModelTraining(\n",
    "                            model=model,\n",
    "                            lr=lr,\n",
    "                            batchsize=batchsize,\n",
    "                            wd=wd,\n",
    "                            dd=dd,\n",
    "                            X_drug_data_train=X_inner_train_data,\n",
    "                            Y_train=Y_inner_train,\n",
    "                            validation_data=(X_val_data, Y_val),\n",
    "                            patience=10,\n",
    "                            fold=fold_num,\n",
    "                            nb_epoch=500\n",
    "                        )\n",
    "                        \n",
    "                        # Evaluate on outer test\n",
    "#                         print(\"Evaluating on outer test set...\")\n",
    "                        X_test_data = list(zip(X_outer_test_feat, X_outer_test_adj))\n",
    "                        log_file_path = f'CarcGC_data/CarcGC_test_fold{fold_num}_lr{lr}_bs{batchsize}_dd{dd}_wd{wd}.log'\n",
    "                        auROC_test, auPR_test, Y_pred_test = ModelEvaluate(\n",
    "                            model=model,\n",
    "                            X_drug_data_test=X_test_data,\n",
    "                            Y_test=Y_outer_test,\n",
    "                            log_file_path=log_file_path\n",
    "                        )\n",
    "                        auROC_folds.append(auROC_test)\n",
    "                        auPR_folds.append(auPR_test)\n",
    "                        \n",
    "                        fold_num += 1\n",
    "                    \n",
    "                    # After all folds for this hyperparameter combination\n",
    "                    mean_auROC = np.mean(auROC_folds)\n",
    "                    std_auROC = np.std(auROC_folds)\n",
    "                    mean_auPR = np.mean(auPR_folds)\n",
    "                    std_auPR = np.std(auPR_folds)\n",
    "#                     print(f\"\\n===== Results for Hyperparameters (lr={lr}, batchsize={batchsize}, dd={dd}, wd={wd}) =====\")\n",
    "#                     print(f\"Mean AUC-ROC: {mean_auROC:.4f} ± {std_auROC:.4f}\")\n",
    "#                     print(f\"Mean AUC-PR: {mean_auPR:.4f} ± {std_auPR:.4f}\")\n",
    "                    \n",
    "                    # Append results to grid_results\n",
    "                    grid_results.append([lr, batchsize, dd, wd, mean_auROC, std_auROC, mean_auPR, std_auPR])\n",
    "                    \n",
    "                    # Increment combination counter\n",
    "                    current_combination += 1\n",
    "    \n",
    "    # After all hyperparameter combinations\n",
    "    # Convert grid_results to DataFrame\n",
    "    grid_df = pd.DataFrame(grid_results, columns=[\n",
    "        'Learning_Rate', 'Batch_Size', 'Dropout', 'Weight_Decay',\n",
    "        'Mean_AUC_ROC', 'Std_AUC_ROC', 'Mean_AUC_PR', 'Std_AUC_PR'\n",
    "    ])\n",
    "    \n",
    "    # Save grid search results to CSV\n",
    "    grid_df.to_csv('/data/home/dbswn0814/2025JCM/model/CarcGC/mgl/gridsearch_results_mgl.csv', index=False)\n",
    "    print(\"\\n===== Grid Search Completed =====\")\n",
    "    print(grid_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carcgc",
   "language": "python",
   "name": "carcgc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
